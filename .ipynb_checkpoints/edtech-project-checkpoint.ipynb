{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "065a4604",
   "metadata": {},
   "source": [
    "# Course-end Project 1 - EdTech\n",
    "## by Ernie Aparicio ea@launchoc.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e4ad2f",
   "metadata": {},
   "source": [
    "#### 1.  The first task is to divide the video into keyframes using Uniform Time Sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b328b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing videos.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def process_video_for_keyframes(video_path, sampling_rate):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Check if video opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {video_path}\")\n",
    "        return\n",
    "\n",
    "    # Get frame rate\n",
    "    frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Calculate sampling interval in frames\n",
    "    sampling_interval = int(frame_rate * sampling_rate)  # sampling_rate in seconds\n",
    "\n",
    "    frame_count = 0\n",
    "    keyframe_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_count % sampling_interval == 0:\n",
    "            # Folder for keyframes\n",
    "            base_folder = os.path.splitext(os.path.basename(video_path))[0]\n",
    "            keyframe_folder = f'keyframes/{base_folder}'\n",
    "            os.makedirs(keyframe_folder, exist_ok=True)\n",
    "\n",
    "            # Save keyframe\n",
    "            keyframe_file = f'{keyframe_folder}/keyframe_{keyframe_count}.jpg'\n",
    "            cv2.imwrite(keyframe_file, frame)\n",
    "            keyframe_count += 1\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "# List of video paths\n",
    "video_paths = ['nptel_ai/howToLearn.mp4', 'nptel_ml/ML.mp4']\n",
    "sampling_rate = 1  # One keyframe every second\n",
    "\n",
    "# Process each video for keyframes\n",
    "for video_path in video_paths:\n",
    "    process_video_for_keyframes(video_path, sampling_rate)\n",
    "\n",
    "print(\"Done processing videos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3042829",
   "metadata": {},
   "source": [
    "#### 2. Assessment of Instructor Presence and Interaction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdac6d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instructor Visibility Fraction in keyframes/ML: 0.9463414634146341\n",
      "Fraction of Full-Screen Presence: 0.0\n",
      "Fraction of PIP Presence: 0.9463414634146341\n",
      "Instructor Visibility Fraction in keyframes/howToLearn: 0.1963882618510158\n",
      "Fraction of Full-Screen Presence: 0.0\n",
      "Fraction of PIP Presence: 0.1963882618510158\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Load the pre-trained SSD MobileNet model from TensorFlow Hub\n",
    "model_handle = 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_640x640/1'\n",
    "model = hub.load(model_handle).signatures['serving_default']\n",
    "\n",
    "def detect_instructor(frame, presence_threshold=0.5):\n",
    "    # Convert the frame to uint8 and process for model input\n",
    "    frame = tf.image.convert_image_dtype(frame, tf.uint8)\n",
    "    input_tensor = tf.expand_dims(frame, 0)\n",
    "\n",
    "    # Model inference\n",
    "    result = model(input_tensor)\n",
    "\n",
    "    # Parse the results\n",
    "    result = {key:value.numpy() for key,value in result.items()}\n",
    "    detection_scores = result[\"detection_scores\"]\n",
    "    detection_classes = result[\"detection_classes\"]\n",
    "    detection_boxes = result[\"detection_boxes\"]\n",
    "\n",
    "    # Check for instructor presence (class 1 is 'person')\n",
    "    for score, clss, box in zip(detection_scores[0], detection_classes[0], detection_boxes[0]):\n",
    "        if score >= presence_threshold and clss == 1:\n",
    "            return True, box\n",
    "    return False, None\n",
    "\n",
    "def is_full_screen(bbox, frame_shape):\n",
    "    frame_height, frame_width, _ = frame_shape\n",
    "    bbox_width = bbox[3] - bbox[1]  # Calculate width of bounding box\n",
    "    bbox_height = bbox[2] - bbox[0]  # Calculate height of bounding box\n",
    "\n",
    "    # Example thresholds: 50% of frame width and 50% of frame height for full-screen\n",
    "    min_full_screen_width = frame_width * 0.5\n",
    "    min_full_screen_height = frame_height * 0.5\n",
    "\n",
    "    if bbox_width >= min_full_screen_width and bbox_height >= min_full_screen_height:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def process_video_segments(segment_folder):\n",
    "    instructor_presence_count = 0\n",
    "    total_frames = 0\n",
    "    full_screen_count = 0\n",
    "    pip_count = 0\n",
    "\n",
    "    for segment_file in os.listdir(segment_folder):\n",
    "        segment_path = os.path.join(segment_folder, segment_file)\n",
    "        frame = cv2.imread(segment_path)\n",
    "        if frame is not None:\n",
    "            total_frames += 1\n",
    "            present, bbox = detect_instructor(frame)\n",
    "            if present:\n",
    "                instructor_presence_count += 1\n",
    "                if is_full_screen(bbox, frame.shape):\n",
    "                    full_screen_count += 1\n",
    "                else:\n",
    "                    pip_count += 1\n",
    "\n",
    "    return instructor_presence_count, total_frames, full_screen_count, pip_count\n",
    "\n",
    "def analyze_video(video_folder):\n",
    "    instructor_presence, total_frames, full_screen, pip = process_video_segments(video_folder)\n",
    "\n",
    "    if total_frames > 0:\n",
    "        fraction_visible = instructor_presence / total_frames\n",
    "        fraction_full_screen = full_screen / total_frames\n",
    "        fraction_pip = pip / total_frames\n",
    "        print(f\"Instructor Visibility Fraction in {video_folder}: {fraction_visible}\")\n",
    "        print(f\"Fraction of Full-Screen Presence: {fraction_full_screen}\")\n",
    "        print(f\"Fraction of PIP Presence: {fraction_pip}\")\n",
    "    else:\n",
    "        print(f\"No frames to analyze in {video_folder}\")\n",
    "\n",
    "# Analyze both videos\n",
    "analyze_video('keyframes/ML')\n",
    "analyze_video('keyframes/howToLearn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5b67ae",
   "metadata": {},
   "source": [
    "#### We can see from the results above our model did a good job scoring instructor visibility, full-screen presence, and partial for each video.  In the ML video, majority of keyframes are powerpoint and in the ML video, majority of screen is the instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0d4e60",
   "metadata": {},
   "source": [
    "#### Next, lets analyze interaction of instructor by using a facial expression model to classify between emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9138ed9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No model config found in the file at <tensorflow.python.platform.gfile.GFile object at 0x7fc28ddb4b50>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the facial expression recognition model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m emotion_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./emotion-detection.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m face_cascade \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mCascadeClassifier(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./haarcascade_frontalface_default.xml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze_facial_expression\u001b[39m(frame):\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.10/site-packages/keras/src/saving/saving_api.py:262\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    255\u001b[0m         filepath,\n\u001b[1;32m    256\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    258\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.10/site-packages/keras/src/saving/legacy/hdf5_format.py:197\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    195\u001b[0m model_config \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mattrs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo model config found in the file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecode\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    201\u001b[0m     model_config \u001b[38;5;241m=\u001b[39m model_config\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: No model config found in the file at <tensorflow.python.platform.gfile.GFile object at 0x7fc28ddb4b50>."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the facial expression recognition model\n",
    "emotion_model = tf.keras.models.load_model('model.h5')\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "def analyze_facial_expression(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_img = gray[y:y + h, x:x + w]\n",
    "        resized = cv2.resize(face_img, (48, 48))\n",
    "        normalized = resized / 255.0\n",
    "        reshaped = np.reshape(normalized, (1, 48, 48, 1))\n",
    "        result = emotion_model.predict(reshaped)\n",
    "\n",
    "        # result will be a list of probabilities for the seven emotions\n",
    "        # handle the result as per your application need\n",
    "\n",
    "    # Add your logic to return the necessary data\n",
    "    pass\n",
    "\n",
    "def process_keyframes(keyframe_folder):\n",
    "    expression_counts = {emotion: 0 for emotion in ['neutral', 'happiness', 'surprise', 'sadness', 'anger', 'disgust', 'fear', 'contempt']}\n",
    "    total_frames = 0\n",
    "\n",
    "    for keyframe_file in os.listdir(keyframe_folder):\n",
    "        keyframe_path = os.path.join(keyframe_folder, keyframe_file)\n",
    "        keyframe = cv2.imread(keyframe_path)\n",
    "        \n",
    "        if keyframe is not None:\n",
    "            total_frames += 1\n",
    "            expressions = analyze_facial_expression(keyframe)\n",
    "            for expression in expression_counts.keys():\n",
    "                expression_counts[expression] += expressions.get(expression, 0)\n",
    "\n",
    "    # Calculate expression percentages\n",
    "    if total_frames > 0:\n",
    "        for expression in expression_counts:\n",
    "            expression_counts[expression] = expression_counts[expression] / total_frames\n",
    "\n",
    "    return expression_counts\n",
    "\n",
    "def analyze_videos(video_folders):\n",
    "    for video_folder in video_folders:\n",
    "        print(f\"Analyzing keyframes in {video_folder}\")\n",
    "        expression_percentages = process_keyframes(video_folder)\n",
    "        print(f\"Facial Expression Percentages for {video_folder}:\")\n",
    "        for expression, percentage in expression_percentages.items():\n",
    "            print(f\"{expression.capitalize()}: {percentage * 100:.2f}%\")\n",
    "        print(f\"Finished analyzing {video_folder}\")\n",
    "\n",
    "# List of video keyframe folders to analyze\n",
    "video_keyframe_folders = ['keyframes/ML', 'keyframes/howToLearn']\n",
    "\n",
    "# Analyze the videos\n",
    "analyze_videos(video_keyframe_folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f742361c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
